{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7ee8898-7859-4322-b927-14ee7261647b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from typing import Dict,Tuple\n",
    "Table_name = \"sesion_11.gold.ventas\"\n",
    "def get_metrics(\n",
    "    Table_name: str\n",
    ")-> Tuple[Dict[str,int],str]:\n",
    "    \n",
    "    delta_table = DeltaTable.forName(spark, Table_name)\n",
    "    history_df = delta_table.history()\n",
    "    \n",
    "    for row in history_df.collect():\n",
    "        #print(row.operation)\n",
    "        if row.operation in (\"MERGE\",\"WRITE\"):\n",
    "            row_metrics = row.asDict().get(\"operationMetrics\",{})\n",
    "\n",
    "            numeric_metrics = {}\n",
    "            for k,v in raw_metrics.items():\n",
    "                #print(f\"{k} : {v}\")\n",
    "                numeric_metrics[k] = int(v)\n",
    "\n",
    "            return numeric_metrics,row.operation\n",
    "\n",
    "get_metrics(\"sesion_11.gold.ventas\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef83c5b5-3f7a-4590-95c6-1a2fe37b718f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,current_timestamp,col\n",
    "\n",
    "def insert_metrics(\n",
    "    metrics_tuple: Tuple[Dict[str,int],str]\n",
    ")-> None:\n",
    "    metrics,Table_name = metrics_tuple\n",
    "    job_id=0\n",
    "    job_run_id=0\n",
    "    task_run_id=0\n",
    "    job_start_time = '2023-01-01 00:00:00'\n",
    "\n",
    "    catalog, schema, table = Table_name.split(\".\")\n",
    "    df_metrics =(\n",
    "        spark.createDataFrame([metrics])\n",
    "        .withColumn(\"job_id\",lit(job_id))\n",
    "        .withColumn(\"job_run_id\",lit(job_run_id))\n",
    "        .withColumn(\"task_run_id\",lit(task_run_id))\n",
    "        .withColumn(\"job_start_time\",lit(job_start_time)).cast(\"timestamp\")\n",
    "        .withColumn(\"job_end_time\",current_timestamp())\n",
    "        .withColumn(\"job_duration_seconds\",col(\"job_end_time\").cast(\"long\")-col(\"job_start_time\").cast(\"long\") )\n",
    "        .withColumn(\"file_bytes\",col(\"numTargetBytesAdded\"))\n",
    "        .withColumn(\"job_status\",\n",
    "                    when(col(\"file_bytes\")>0,lit(\"Success\") )\n",
    "                    .otherwise(lit(\"Failed\"))\n",
    "                    )\n",
    "        .withColumn(\"table\",lit(table))\n",
    "        .withColumn(\"layer\",lit(chema))\n",
    "        .withColumn(\"row_in\",col(\"numTargetRowsInserted\"))\n",
    "        .withColumn(\"row_inserted\",col(\"numTargetRowsInserted\"))\n",
    "        .withColumn(\"row_update\",col(\"numTargetRowsUpdated\"))\n",
    "        .withColumn(\"row_delete\",col(\"numTargetRowsDeletedd\"))\n",
    "        .withColumn(\"merge_duration_seconds\",lit(\"executionTimeMs\")/lit(1000))\n",
    "        .select(\n",
    "            \"job_id\",\n",
    "            \"job_run_id\",\n",
    "            \"task_run_id\",\n",
    "            \"job_start_time\",\n",
    "            \"job_end_time\",\n",
    "            \"job_duration_seconds\",\n",
    "            \"file_bytes\",\n",
    "            \"job_status\",\n",
    "            \"table\",\n",
    "            \"layer\",\n",
    "            \"row_in\",\n",
    "            \"row_inserted\",\n",
    "            \"row_update\",\n",
    "            \"row_delete\",\n",
    "            \"merge_duration_seconds\"\n",
    "\n",
    "        )\n",
    "        \n",
    "    )\n",
    "    \n",
    "    #display(df_metrics)\n",
    "    delta_table = DeltaTable.forName(spark, \"sesion_11.gold.ventas\")\n",
    "\n",
    "    merge = (\n",
    "        delta_table.alias(\"m\")\n",
    "        .merge(\n",
    "            df_metrics.alias(\"in\"),\n",
    "            lit(False) \n",
    "        )\n",
    "        .whenNotMatchedInsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e3a60a9b-1f7e-4d76-9bcd-aace72f4a331",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-09-09 20_31_18",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
